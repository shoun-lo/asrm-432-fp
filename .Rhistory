library(tidyverse)
library(RCurl)
library(rpart)
library(randomForest)
#importing data from repo
url1 = getURL("https://raw.githubusercontent.com/shoun-lo/asrm-432-fp/main/train.txt")
train_data = read_delim(url1, delim = "\t", escape_double = FALSE, col_names = FALSE, trim_ws = TRUE)
url2 = getURL("https://raw.githubusercontent.com/shoun-lo/asrm-432-fp/main/predicts.txt")
test_data = read_delim(url2, delim = "\t", escape_double = FALSE, col_names = FALSE, trim_ws = TRUE)
url3 = getURL("https://raw.githubusercontent.com/shoun-lo/asrm-432-fp/main/targets.txt")
target_data = read_delim(url3, delim = "\t", escape_double = FALSE, col_names = FALSE, trim_ws = TRUE)
#set seed to be 7
set.seed(7)
#first hundred observations.
train_selected = train_data[1:100, ]
#apply bagging method
bagging = randomForest(as.factor(train_selected$X86) ~., data = train_selected,
mtry = 85, importance = TRUE)
#plot to the best ntrees
plot(bagging, col = "darkorange")
#choose trees 330-350
bagging = randomForest(as.factor(train_selected$X86) ~., data = train_selected,
ntree = 340, mtry = 85, importance = TRUE)
#bagging predictions
bagging_pred = predict(bagging, newdata = test_data)
plot(bagging_pred)
#confusion matrix and accuracy (training data)
conf_tab = table(Predicted = bagging_pred, Actual = train_data$X86[1:4000])
sum(diag(conf_tab)/nrow(train_data[1:4000,]))
#confusion matrix and accuracy (target data)
conf_tab2 = table(Predicted = bagging_pred, Actual = target_data$X1)
sum(diag(conf_tab2)/nrow(target_data))
#importance of vars.
importance = importance(bagging)
importance[, 3:4] = abs(importance[, 3:4])
sort(importance[,3], decreasing = TRUE)
importance = sort(importance[,3], decreasing = TRUE)
#importance of vars. (Mean Decreasing Accuracy)
importance = importance(bagging)
importance[, 3:4] = abs(importance[, 3:4])
importance = sort(importance[,3], decreasing = TRUE)
head(importance, 5)
head(importance, 10)
#confusion matrix and accuracy (training data) (go over with prof or ta)
conf_tab = table(Predicted = bagging_pred, Actual = train_data$X86[1:4000])
sum(diag(conf_tab)) / sum(conf_tab)
sum(diag(conf_tab2)) / sum(conf_tab2)
#random forest
random_forest = randomForest(as.factor(train_selected$X86) ~.,
data = train_selected, importance = TRUE)
random_forest
bagging
bagging$confusion
bagging$confusion
1-bagging$err.rate
bagging$err.rate
view(bagging$confusion)
94 / 100
#confusion matrix and accuracy (training data) (go over with prof or ta)
bagging$confusion
#confusion matrix and accuracy (training data) (go over with prof or ta)
bagging$confusion[, 1:2]
#confusion matrix and accuracy (training data) (go over with prof or ta)
conf_tab = bagging$confusion[, 1:2]
bagging_pred$
#confusion matrix and accuracy (training data) (go over with prof or ta)
conf_tab = bagging$confusion[, 1:2]
sum(diag(conf_tab)) / sum(conf_tab)
#or
conf_tab = table(Predicted = bagging_pred, Actual = train_data$X86[1:4000])
sum(diag(conf_tab)) / sum(conf_tab)
#or
conf_tab = table(Predicted = bagging_pred, Actual = train_data$X86[1:4000])
#or
conf_tab = table(Predicted = bagging_pred, Actual = train_data$X86[1:4000])
#bagging predictions
bagging_pred = predict(bagging, newdata = test_data)
#confusion matrix and accuracy (training data) (go over with prof or ta)
conf_tab = bagging$confusion[, 1:2]
#or
conf_tab = table(Predicted = bagging_pred, Actual = train_data$X86[1:4000])
sum(diag(conf_tab)) / sum(conf_tab)
#confusion matrix and accuracy (training data) (go over with prof or ta)
conf_tab = bagging$confusion[, 1:2]
sum(diag(conf_tab)) / sum(conf_tab)
#or
conf_tab = table(Predicted = bagging_pred, Actual = train_data$X86[1:4000])
sum(diag(conf_tab)) / sum(conf_tab)
View(bagging)
length(bagging_pred)
#random forest predictions on testing data
random_forest_pred = predict(random_forest, newdata = test_data)
#apply random forest onto training data
random_forest = randomForest(as.factor(train_selected$X86) ~.,
data = train_selected, importance = TRUE)
random_forest
#tune mtry
tuned = tuneRF(x = train_data[, -86], y = train_data$X86, ntreeTry = 500, stepFactor = 1.5)
#tune mtry
tuned = tuneRF(x = train_data[, -86], y = as.factor(train_data$X86), ntreeTry = 500, stepFactor = 1.5)
#tune mtry
tuned = tuneRF(x = train_data[, -86], y = as.factor(train_data$X86), ntreeTry = 500, stepFactor = 1.5)
?tuneRF
#tune mtry
tuned = tuneRF(x = train_data[, -86], y = as.factor(train_data$X86), ntreeTry = 500, stepFactor = 1.5)
#tune mtry
tuned = tuneRF(x = train_data[, -86], y = as.factor(train_data$X86), ntreeTry = 300, stepFactor = 1.5)
#tune mtry
tuned = tuneRF(x = train_data[, -86], y = as.factor(train_data$X86), ntreeTry = 340, stepFactor = 1.5)
#tune mtry
tuned = tuneRF(x = train_data[, -86], y = as.factor(train_data$X86), ntreeTry = 340, mtryStart = 42, stepFactor = 1.5)
#optimal mtry value
optimal_mtry = tuned$mtry[which.min(tuned$err.rate[, "OOB"])]
#optimal mtry value
optimal_mtry = tuned$mtry[which.min(tuned$err.rate[, "OOB"])]
#tune mtry
tuned = tuneRF(x = train_data[, -86], y = as.factor(train_data$X86), ntreeTry = 340, mtryStart = 42, stepFactor = 1.5)
View(tuned)
View(tuned)
View(tuned)
View(tuned)
#tune mtry
tuned = tuneRF(x = train_data[, -86], y = as.factor(train_data$X86), ntreeTry = 340, mtryStart = 85, stepFactor = 1.5)
?tuneRF
View(tuned)
View(tuned)
View(tuned)
#tune mtry
tuned = tuneRF(x = train_data[, -86], y = as.factor(train_data$X86),
ntreeTry = 340, mtryStart = 42, stepFactor = 1.5)
knitr::opts_chunk$set(echo=TRUE)
# inputs to download file
fileLocation <- "https://pjreddie.com/media/files/mnist_train.csv"
numRowsToDownload_2 <- 20000
localFileName_2 <- paste0("mnist_first", numRowsToDownload_2, ".RData")
# download the data and add column names
mnist_2 <- read.csv(fileLocation, nrows = numRowsToDownload_2)
numColsMnist_2 <- dim(mnist_2)[2]
colnames(mnist_2) <- c("Digit", paste("Pixel", seq(1:(numColsMnist_2 - 1)), sep = ""))
# save file
# in the future we can read in from the local copy instead of having to redownload
if(file.exists(localFileName_2)){
file.remove(localFileName_2)
}
save(mnist_2, file = localFileName_2)
# you can load the data with the following code
load(file = localFileName_2)
# keep rows corresponding to digits 1, 6, and 7
digitLevels_2 <- c(1, 6, 7)
mnistFiltered_2 <- mnist_2[which(mnist_2$Digit %in% digitLevels_2),]
n_mnistFiltered_2 <- dim(mnistFiltered_2)[1]
# set digit to be a factor
mnistFiltered_2$Digit <- as.factor(mnistFiltered_2$Digit)
# set seed
set.seed(7)
# training set parameters
train_percentage <- 0.75
train_size <- floor(train_percentage*n_mnistFiltered_2)
train_indices <- sample(x = 1:n_mnistFiltered_2, size = train_size)
# separate dataset into train and test
train_mnist <- mnistFiltered_2[train_indices,]
test_mnist <- mnistFiltered_2[-train_indices,]
# extract out Pixel variables for ease from training and testing data sets
train_mnist_X <- train_mnist[,which(colnames(train_mnist) != "Digit")]
test_mnist_X <- test_mnist[,which(colnames(test_mnist) != "Digit")]
#pca on training data
pca_pixel = prcomp(train_mnist_X)
#extract the first 8 PCs from training and print
pca_pixel_test = pca_pixel$x[,1:8]
head(pca_pixel_test, 5)
#generate score matrix for testing
predict_pixel = predict(pca_pixel, newdata = test_mnist_X)
#extract first 8 PCs from testing and print
pca_pixel_test = pca_pixel_test[, 1:8]
head(pca_pixel_test, 5)
#set seed
set.seed(7)
# Define K_Means_custom function
K_Means_custom = function(K, inputData_X, maxIter) {
#euclidean distance
euclidean_distance = function(x1, x2) {
sqrt(sum((x1 - x2)^2))
}
#step 1. randomly assign obs. into clusters
initial_clusters = sample(K, nrow(inputData_X), replace = TRUE)
#step 2. compute centroid at each cluster
compute_centroids = function(data, clusters, K) {
#initialize centroids
centroids = matrix(0, nrow = K, ncol = ncol(data))
#compute at each cluster
for (k in 1:K) {
centroids[k, ] = colMeans(data[clusters == k, ])
}
return(centroids)
}
centroids = compute_centroids(inputData_X, initial_clusters, K)
#step 3. assign each cluster with the closest centroid
assign_clusters = function(data, centroids) {
#initialize clusters
clusters = rep(0, nrow(data))
#assign each cluster with closest centroid
for (i in 1:nrow(data)) {
min_dist = Inf
for (k in 1:nrow(centroids)) {
dist = euclidean_distance(data[i, ], centroids[k, ])
if (dist < min_dist) {
min_dist = dist
clusters[i] = k
}
}
}
return(clusters)
}
#initial cluster assignment
clusters = assign_clusters(inputData_X, centroids)
#repeat step 2 and 3 until maxIter or converge.
for (iter in 1:maxIter) {
original_clusters = clusters
centroids = compute_centroids(inputData_X, clusters, K)
clusters = assign_clusters(inputData_X, centroids)
if (all(clusters == original_clusters)) {
break
}
}
# Return the final cluster assignments
inputData_K_Means_assigned = cbind(clusters, inputData_X)
return(inputData_K_Means_assigned)
}
results = as.tibble(results)
library(tidyverse)
results = as.tibble(results)
results = K_Means_custom(3, pca_pixel$x, maxIter = 250)
results = as_tibble(results)
results = results|>
mutate(cluster = as.factor(clusters))
results = results|>
mutate(cluster = as.factor(clusters)) |>
results$clusters = as.factor(results$clusters)
results$clusters = as.factor(results$clusters)
levels(results$clusters) = levels(results$clusters)
levels(results$clusters) = levels(train_mnist$Digit)
View(results)
confusion_matrix <- table(results[, 1], train_mnist$Digit)
confusion_matrix <- table(results$clusters, train_mnist$Digit)
confusion_matrix
results = kmeans(x = pca_pixel$x, centers = 3, nstart = 20)
results2 = kmeans(x = pca_pixel$x, centers = 3, nstart = 20)
confusion_matrix2 = table(results2$cluster, train_mnist$Digit)
confusion_matrix2
level(results2$cluster) = level(as.factor(train_mnist$Digit))
levels(results2$cluster) = levels(as.factor(train_mnist$Digit))
confusion_matrix2 = table(results2$cluster, train_mnist$Digit)
confusion_matrix2
results2 = kmeans(x = pca_pixel$x, centers = 3, nstart = 20)
results2 = kmeans(x = pca_pixel$x, centers = 3, nstart = 20)
results2$cluster
levels(results2$cluster) = levels(as.factor(train_mnist$Digit))
confusion_matrix2 = table(results2$cluster, train_mnist$Digit)
confusion_matrix2
View(results2)
View(results2$cluster)
View(results2$cluster[,1])
View(results2$cluster[1])
results2$cluster[1]
results2$cluster[2]
results2$cluster
results2$cluster
results2$cluster
set.seed(7)
results2 = kmeans(x = pca_pixel$x, centers = 3, nstart = 20)
results2$cluster
results2$cluster[,1]
levels(results2$cluster)
confusion_matrix2 = table(results2$cluster, train_mnist$Digit)
confusion_matrix2
results = K_Means_custom(3, pca_pixel$x, maxIter = 250)
results = K_Means_custom(3, pca_pixel$x, maxIter = 250)
results = as_tibble(results)
confusion_matrix = table(results$clusters, train_mnist$Digit)
confusion_matrix
confusion_matrix
